import os
from dotenv import load_dotenv
from sentence_transformers import CrossEncoder
import re
from vertexai import rag
from vertexai.generative_models import Tool, GenerativeModel

# ================== ç’°å¢ƒè®Šæ•¸ ==================
load_dotenv()
PROJECT_ID = os.getenv("GCP_PROJECT")
LOCATION = os.getenv("GCP_LOCATION", "us-central1")
RAG_CORPUS_NAME = os.getenv("RAG_CORPUS_NAME")

if not PROJECT_ID or not RAG_CORPUS_NAME:
    raise ValueError("âŒ è«‹åœ¨ .env è¨­å®š GCP_PROJECT èˆ‡ RAG_CORPUS_NAME")

def clean_output(text: str) -> str:
    if not text:
        return ""
    # ç§»é™¤å¸¸è¦‹å†—é¤˜é–‹é ­
    text = re.sub(r"^ä»¥ä¸‹æ˜¯.*\n?", "", text)
    text = re.sub(r"^å¥½çš„ï¼Œæˆ‘ä¾†.*\n?", "", text)
    # ç§»é™¤ã€Œä¿®æ­£èªªæ˜ã€ã€ã€Œä¿®æ­£å¾Œã€ç­‰å­—çœ¼
    text = re.sub(r"(ä¿®æ­£èªªæ˜åŠç†ç”±ï¼š|ä¿®æ­£å¾Œçš„é¢¨éšªåˆ†æï¼š|ä¿®æ­£å¾Œçš„æ‘˜è¦ï¼š|ä¿®æ­£å¾Œçš„åˆ†æï¼š)", "", text)
    return text.strip()


# ================== åˆå§‹åŒ– RAG + Gemini ==================
rag_retrieval_config = rag.RagRetrievalConfig(
    top_k=10,
    filter=rag.Filter(vector_distance_threshold=0.5)
)

rag_tool = Tool.from_retrieval(
    retrieval=rag.Retrieval(
        source=rag.VertexRagStore(
            rag_resources=[rag.RagResource(rag_corpus=RAG_CORPUS_NAME)],
            rag_retrieval_config=rag_retrieval_config
        )
    )
)

# ç¬¬ä¸€å±¤ Geminiï¼ˆç”Ÿæˆåˆæ­¥ç­”æ¡ˆï¼‰
gen_model_primary = GenerativeModel(
    model_name="gemini-2.0-flash-001",
    tools=[rag_tool]
)

# ç¬¬äºŒå±¤ Geminiï¼ˆè¤‡æ ¸ç­”æ¡ˆï¼‰
gen_model_reviewer = GenerativeModel(
    model_name="gemini-2.0-flash-001"
)

# ================== åˆå§‹åŒ– reranker ==================
reranker = CrossEncoder("BAAI/bge-reranker-large")

# ================== RAG æª¢ç´¢ + rerank ==================
def rag_search_with_rerank(query: str, n=10, top_k=3):
    rag_response = rag.retrieval_query(
        rag_resources=[rag.RagResource(rag_corpus=RAG_CORPUS_NAME)],
        text=query,
        rag_retrieval_config=rag_retrieval_config,
    )

    candidates = []
    if rag_response and getattr(rag_response, "contexts", None):
        for ctx in rag_response.contexts.contexts[:n]:
            candidates.append((ctx.text, {"law_name": "RAG"}, ctx.score, "VertexRAG"))

    if not candidates:
        return []

    # rerank
    pairs = [(query, doc) for doc, _, _, _ in candidates]
    scores = reranker.predict(pairs)
    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)

    return ranked[:top_k]

# ================== Gemini é›™å±¤å›ç­” ==================
def generate_answer_with_review(query, context_texts, sources):
    # ç¬¬ä¸€å±¤ï¼šç”Ÿæˆåˆæ­¥ç­”æ¡ˆ
    prompt_primary = (
        "ä½ æ˜¯é¦™æ¸¯æ³•å¾‹è¼”åŠ©åŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ³•å¾‹æ¢æ–‡å›ç­”å•é¡Œã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”\n"
        "2. è©³ç´°åˆ†æï¼Œé‡é»çªå‡º\n"
        "3. é€™ä¸æ˜¯æ³•å¾‹æ„è¦‹ï¼Œåƒ…ä¾›åƒè€ƒ\n\n"
        f"ç›¸é—œæ³•å¾‹æ¢æ–‡ï¼š\n{chr(10).join(context_texts[:3])}\n\n"
        f"å•é¡Œï¼š{query}\n\n"
        "è«‹å›ç­”ï¼š"
    )
    response_primary = gen_model_primary.generate_content(prompt_primary)
    draft_answer = response_primary.text

    # ç¬¬äºŒå±¤ï¼šè¤‡æ ¸ç­”æ¡ˆï¼ˆè¦æ±‚ä¹¾æ·¨è¼¸å‡ºï¼‰
    prompt_review = (
        "ä½ æ˜¯ä¸€ä½åš´è¬¹çš„æ³•å¾‹å¯©æ ¸åŠ©æ‰‹ã€‚è«‹ç›´æ¥è¼¸å‡ºæœ€çµ‚ç­”æ¡ˆï¼Œä¸è¦åŒ…å«ã€Œä»¥ä¸‹æ˜¯ã€ã€ã€Œä¿®æ­£å¾Œã€ã€ã€Œä¿®æ­£èªªæ˜ã€ç­‰å­—çœ¼ï¼Œä¹Ÿä¸è¦æè¿°å¯©æ ¸éç¨‹ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. æª¢æŸ¥å›ç­”æ˜¯å¦æº–ç¢ºã€æ˜¯å¦æœ‰éºæ¼æˆ–éŒ¯èª¤\n"
        "2. è‹¥æ–‡æœ¬æœªæ¶µè“‹ï¼Œè«‹æ˜ç¢ºæ¨™è¨»ã€Œä¸ç¢ºå®š/è³‡æ–™ä¸è¶³ã€\n"
        "3. ä¿æŒç¹é«”ä¸­æ–‡ï¼Œæ¢åˆ—å¼é‡é»ï¼Œé¿å…å†—é•·\n\n"
        f"å•é¡Œï¼š{query}\n\n"
        f"ç›¸é—œæ³•å¾‹æ¢æ–‡ï¼š\n{chr(10).join(context_texts[:3])}\n\n"
        f"åˆæ­¥å›ç­”ï¼š\n{draft_answer}\n\n"
        "è«‹è¼¸å‡ºä¹¾æ·¨ã€æ­£å¼çš„æœ€çµ‚ç­”æ¡ˆï¼š"
    )
    response_review = gen_model_reviewer.generate_content(prompt_review)
    final_answer = clean_output(response_review.text)

    return f"{final_answer}\n\nğŸ“š ä¾†æºï¼š\n" + "\n".join(sources)


# ================== ä¸»ç¨‹å¼ ==================
if __name__ == "__main__":
    print("âœ… RAG Pipeline (Vertex AI + rerank + Gemini è¤‡æ ¸) å·²å•Ÿå‹•ï¼ˆè¼¸å…¥ exit é›¢é–‹ï¼‰")

    while True:
        query = input("\nâ“ è«‹è¼¸å…¥å•é¡Œ: ").strip()
        if query.lower() in ["exit", "quit", "q"]:
            print("ğŸ‘‹ å†è¦‹ï¼")
            break

        ranked = rag_search_with_rerank(query, n=10, top_k=3)

        if not ranked:
            print("âš ï¸ æ²’æœ‰æª¢ç´¢åˆ°ç›¸é—œå…§å®¹")
            continue

        context_texts = [doc for (doc, _, _, _), _ in ranked]
        sources = [f"- {meta.get('law_name','')}" for (_, meta, _, _), _ in ranked]

        answer = generate_answer_with_review(query, context_texts, sources)
        print("\nğŸ§  Gemini æœ€çµ‚å›ç­”ï¼ˆè¤‡æ ¸å¾Œï¼‰ï¼š")
        print(answer)
