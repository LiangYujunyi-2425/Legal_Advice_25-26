import os
import requests
import streamlit as st
import chromadb
from contract_ingest import load_contract, split_into_clauses, GTEEmbeddingFunction
from docx import Document
from io import BytesIO
import json
from datetime import datetime
import pickle
from rank_bm25 import BM25Okapi
import jieba
from sentence_transformers import SentenceTransformer, CrossEncoder
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--generator", type=str, default="mistral:7b-instruct")
parser.add_argument("--verifier", type=str, default="qwen3:8b")
args = parser.parse_args()

GENERATOR_MODEL = args.generator
VERIFIER_MODEL = args.verifier

# ==========================
# Ollama è¨­å®š
# ==========================
OLLAMA_API_URL = "http://localhost:11434/api/generate"

def call_ollama(model_name, prompt, max_tokens=500):
    payload = {
        "model": model_name,
        "prompt": prompt,
        "options": {"temperature": 0.3},
        "stream": False
    }
    resp = requests.post(OLLAMA_API_URL, json=payload)
    if resp.status_code == 200:
        return resp.json().get("response", "").strip()
    else:
        return f"âŒ Ollama è«‹æ±‚å¤±æ•—: {resp.text}"

# ==========================
# åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
# ==========================
client = chromadb.PersistentClient(path="./chroma_db")
laws_collection = client.get_collection(
    name="hk_cap4_laws", embedding_function=GTEEmbeddingFunction("thenlper/gte-large-zh")
)
contracts_collection = client.get_or_create_collection(
    name="contracts", embedding_function=GTEEmbeddingFunction("thenlper/gte-large-zh")
)

# ==========================
# BM25 (è¼‰å…¥ç´¢å¼•)
# ==========================
with open("bm25_index.pkl", "rb") as f:
    bm25_data = pickle.load(f)
bm25 = bm25_data["bm25"]
bm25_chunks = bm25_data["chunks"]

embedder = SentenceTransformer("thenlper/gte-large-zh")
reranker = CrossEncoder("BAAI/bge-reranker-large")

# ==========================
# Hybrid Search
# ==========================
def hybrid_search(query: str, n=10):
    vector_results = laws_collection.query(
        query_texts=[query],
        n_results=n,
        include=["documents", "metadatas", "distances"]
    )
    vector_docs = vector_results["documents"][0]
    vector_metas = vector_results["metadatas"][0]
    vector_scores = vector_results["distances"][0]

    vector_candidates = [
        (doc, meta, 1 - score, "Chroma")
        for doc, meta, score in zip(vector_docs, vector_metas, vector_scores)
    ]

    tokenized_query = list(jieba.cut(query))
    bm25_scores = bm25.get_scores(tokenized_query)
    top_idx = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:n]

    bm25_candidates = [
        (bm25_chunks[i]["text"], bm25_chunks[i], bm25_scores[i], "BM25") for i in top_idx
    ]

    merged = {}
    for doc, meta, score, source in vector_candidates + bm25_candidates:
        if doc not in merged or score > merged[doc][1]:
            merged[doc] = (meta, score, source)

    return [(doc, meta, score, source) for doc, (meta, score, source) in merged.items()]

def rerank(query, candidates, top_k=3):
    pairs = [(query, doc) for doc, _, _, _ in candidates]
    scores = reranker.predict(pairs)
    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
    return ranked[:top_k]

# ==========================
# é›™ LLM Pipeline
# ==========================
def generate_answer(query, context_texts):
    prompt = (
        "ä½ æ˜¯ä¸€ä½é¦™æ¸¯æ³•å¾‹è¼”åŠ©åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ¢æ–‡ç”Ÿæˆåˆæ­¥å›ç­”ï¼š\n\n"
        f"æ¢æ–‡ï¼š\n{chr(10).join(context_texts)}\n\nå•é¡Œï¼š{query}"
    )
    return call_ollama(GENERATOR_MODEL, prompt, max_tokens=512)

def verify_answer(query, draft_answer, context_texts):
    prompt = (
        "ä½ æ˜¯ä¸€ä½åš´è¬¹çš„æ³•å¾‹å¯©æ ¸åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯åˆæ­¥å›ç­”èˆ‡æ³•å¾‹æ¢æ–‡ï¼š\n\n"
        f"å•é¡Œï¼š{query}\n\n"
        f"åˆæ­¥å›ç­”ï¼š{draft_answer}\n\n"
        f"æ¢æ–‡ï¼š\n{chr(10).join(context_texts)}\n\n"
        "è«‹æª¢æŸ¥ä¸¦ä¿®æ­£éŒ¯èª¤ï¼Œè£œå……éºæ¼ï¼Œä¸¦åŠ å¼·å¼•ç”¨ï¼Œä¿æŒç¹é«”ä¸­æ–‡ã€‚"
    )
    return call_ollama(VERIFIER_MODEL, prompt, max_tokens=700)

# ==========================
# Streamlit UI
# ==========================
st.set_page_config(page_title="åˆç´„ + æ³•å¾‹åˆ†æç³»çµ±", layout="wide")
st.title("ğŸ“‘ æœ¬åœ°åˆç´„ + æ³•å¾‹åˆ†æç³»çµ±ï¼ˆOllama + é›™ LLM + RAGï¼‰")

uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šå‚³åˆç´„ (PDF / DOCX / TXT)", type=["pdf", "docx", "txt"])

if uploaded_file:
    with open(f"./contracts/{uploaded_file.name}", "wb") as f:
        f.write(uploaded_file.getbuffer())
    text = load_contract(f"./contracts/{uploaded_file.name}")
    clauses = split_into_clauses(text, max_len=600)

    st.success(f"âœ… æˆåŠŸè¼‰å…¥åˆç´„ï¼Œå…± {len(clauses)} å€‹æ¢æ¬¾")

    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“– æ¢æ¬¾é€æ¢åˆ†æ", "ğŸ“Œ åˆç´„æ‘˜è¦", "âš ï¸ é¢¨éšªé‡é»", "âš–ï¸ æ³•å¾‹æª¢ç´¢"])

    clause_analyses = []

    # æ¢æ¬¾é€æ¢åˆ†æ
    with tab1:
        for i, clause in enumerate(clauses):
            with st.spinner(f"åˆ†æç¬¬ {i+1} æ¢æ¬¾ä¸­..."):
                draft = generate_answer(clause, [clause])
                analysis = verify_answer(clause, draft, [clause])
                clause_analyses.append((clause, analysis))
                st.markdown(f"### æ¢æ¬¾ {i+1}")
                st.info(clause)
                st.write(analysis)

    # åˆç´„æ‘˜è¦
    with tab2:
        with st.spinner("æ­£åœ¨ç”Ÿæˆåˆç´„æ‘˜è¦..."):
            draft_summary = generate_answer("è«‹ç¸½çµæ­¤åˆç´„", [text[:6000]])
            summary = verify_answer("è«‹ç¸½çµæ­¤åˆç´„", draft_summary, [text[:6000]])
        st.subheader("ğŸ“Œ åˆç´„æ‘˜è¦")
        st.write(summary)

    # é¢¨éšªé‡é»
    with tab3:
        with st.spinner("æ­£åœ¨ç”Ÿæˆé¢¨éšªé‡é»..."):
            draft_risks = generate_answer("è«‹æ‰¾å‡ºåˆç´„ä¸­çš„é¢¨éšª", [text[:6000]])
            risks = verify_answer("è«‹æ‰¾å‡ºåˆç´„ä¸­çš„é¢¨éšª", draft_risks, [text[:6000]])
        st.subheader("âš ï¸ é¢¨éšªé‡é»")
        st.write(risks)

    # æ³•å¾‹æª¢ç´¢
    with tab4:
        query = st.text_input("è¼¸å…¥æ³•å¾‹å•é¡Œï¼ˆçµåˆ RAG æª¢ç´¢ï¼‰")
        if query:
            candidates = hybrid_search(query, n=10)
            reranked = rerank(query, candidates, top_k=3)
            context_texts = [doc for (doc, _, _, _), _ in reranked]

            draft_answer = generate_answer(query, context_texts)
            final_answer = verify_answer(query, draft_answer, context_texts)

            st.subheader("ğŸ§  æ³•å¾‹å›ç­”")
            st.write(final_answer)

# å ±å‘Šä¸‹è¼‰
def generate_word_report(summary, risks, clause_analyses):
    doc = Document()
    doc.add_heading("åˆç´„åˆ†æå ±å‘Š", level=1)
    doc.add_heading("ğŸ“Œ åˆç´„æ‘˜è¦", level=2)
    doc.add_paragraph(summary if summary else "ç„¡æ‘˜è¦")
    doc.add_heading("âš ï¸ æ½›åœ¨é¢¨éšª", level=2)
    doc.add_paragraph(risks if risks else "æœªæª¢æ¸¬åˆ°æ˜é¡¯é¢¨éšªã€‚")
    doc.add_heading("ğŸ“‘ æ¢æ¬¾é€æ¢åˆ†æ", level=2)
    for i, (clause, analysis) in enumerate(clause_analyses, 1):
        doc.add_heading(f"æ¢æ¬¾ {i}", level=3)
        doc.add_paragraph(clause, style="Intense Quote")
        doc.add_paragraph(f"ğŸ” åˆ†æ: {analysis}")
    buf = BytesIO()
    doc.save(buf)
    buf.seek(0)
    return buf

def save_json_report(summary, risks, clause_analyses, filename="contract_analysis.json"):
    data = {
        "generated_at": datetime.now().isoformat(),
        "summary": summary,
        "risks": risks,
        "clauses": [{"clause": c, "analysis": a} for c, a in clause_analyses]
    }
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return filename

if uploaded_file:
    if st.button("ğŸ“¥ ä¸‹è¼‰ Word å ±å‘Š"):
        buf = generate_word_report(summary, risks, clause_analyses)
        st.download_button(
            label="â¬‡ï¸ ä¸‹è¼‰åˆç´„åˆ†æå ±å‘Š (Word)",
            data=buf,
            file_name="contract_report.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
    if st.button("ğŸ“¥ ä¸‹è¼‰ JSON å ±å‘Š"):
        json_file = save_json_report(summary, risks, clause_analyses)
        st.success(f"âœ… JSON è¨˜éŒ„å·²ä¿å­˜åˆ°æœ¬åœ°ï¼š{json_file}")
