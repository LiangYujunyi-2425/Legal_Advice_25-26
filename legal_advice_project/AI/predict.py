# app/predict.py
import os
import re
from typing import Dict, Any, List
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import asyncio
import time
import json

# Google Vertex AI (use vertexai GenerativeModel for endpoint calls)
try:
    import vertexai
    from vertexai.generative_models import GenerativeModel
except Exception:
    vertexai = None
    GenerativeModel = None


# Configure via environment variables
GCP_PROJECT = os.environ.get("GCP_PROJECT")
GCP_LOCATION = os.environ.get("GCP_LOCATION")
# REQUIRED: Vertex Endpoint ID
VERTEX_ENDPOINT_ID = os.environ.get("VERTEX_ENDPOINT_ID")


# Global model instance
generative_model = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    global generative_model

    # require vertexai package
    if GenerativeModel is None:
        raise RuntimeError("vertexai package not available. Install google-cloud-vertexai in requirements.")

    # åˆå§‹åŒ– vertexai
    try:
        vertexai.init(project=GCP_PROJECT, location=GCP_LOCATION)
    except Exception:
        # vertexai.init may raise if env vars missing; still proceed to construct endpoint_path
        pass

    # å»ºç«‹ Endpoint çš„å®Œæ•´è·¯å¾‘
    if not VERTEX_ENDPOINT_ID:
        raise RuntimeError("VERTEX_ENDPOINT_ID environment variable must be set")

    endpoint_path = f"projects/{GCP_PROJECT}/locations/{GCP_LOCATION}/endpoints/{VERTEX_ENDPOINT_ID}"

    try:
        # ä½¿ç”¨ GenerativeModel åˆå§‹åŒ–ï¼ˆç›´æ¥å° endpoint å‘¼å«ï¼‰
        generative_model = GenerativeModel(endpoint_path)
        print(f"âœ… GenerativeModel initialized for endpoint: {endpoint_path}")
    except Exception as e:
        raise RuntimeError(f"Failed to initialize GenerativeModel with endpoint '{endpoint_path}': {e}")
    
    yield
    
    print("ğŸ›‘ æœå‹™é—œé–‰ï¼Œé‡‹æ”¾è³‡æº")


# å»ºç«‹ FastAPI æ‡‰ç”¨ï¼Œä¸¦æŒ‡å®š lifespan
app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æˆ–æŒ‡å®š ["https://your-frontend.com"]
    allow_credentials=True,
    allow_methods=["*"],  # åŒ…å« OPTIONS
    allow_headers=["*"],
)


def extract_answer(text: str) -> str:
    """åªæŠ½å– <answer> ... </answer> ä¹‹é–“çš„å…§å®¹"""
    m = re.search(r"<answer>(.*?)</answer>", text, re.DOTALL)
    return m.group(1).strip() if m else text.strip()

def sanitize_output(text: str) -> str:
    # ç§»é™¤ <instruction> ... </instruction>
    text = re.sub(r"<instruction>.*?</instruction>", "", text, flags=re.DOTALL)
    # ç§»é™¤ <question> ... </question>
    text = re.sub(r"<question>.*?</question>", "", text, flags=re.DOTALL)
    # ç§»é™¤å¤šé¤˜ç©ºç™½
    return text.strip()



def _vertex_predict_sync(prompt: str, max_output_tokens: int = 256, **predict_kwargs) -> str:
    """
    åŒæ­¥å‘¼å« Vertex Endpoint é€²è¡Œé æ¸¬
    ä½¿ç”¨ Endpoint.predict() æ­é… Gemini 2.5 Flash æ¨¡å‹
    """
    # Use the vertexai GenerativeModel (endpoint) to generate content
    if generative_model is None:
        return "[vertex predict error] generative model not initialized"

    try:
        final_prompt = prompt
        # Call generate_content with a simple string prompt (matches provided example)
        resp = generative_model.generate_content(final_prompt)
        if hasattr(resp, "text") and resp.text:
            return resp.text

        # Try to extract common fields if 'text' not present
        try:
            obj = getattr(resp, "__dict__", {})
            for k in ("text", "content", "outputs", "candidates"):
                if k in obj and obj[k]:
                    return str(obj[k])
        except Exception:
            pass

        return str(resp)
    except Exception as e:
        return f"[vertex predict error] {str(e)}"

async def llm_generate(prompt: str, max_new_tokens: int = 256, **predict_kwargs) -> str:
    # Run blocking Vertex call in threadpool to avoid blocking event loop
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, _vertex_predict_sync, prompt, max_new_tokens, **predict_kwargs)

# ---- Shared memory/context ----
class Memory:
    def __init__(self):
        self.messages: List[Dict[str, str]] = []
        self.notes: Dict[str, Any] = {}
    def add(self, role: str, content: str):
        self.messages.append({"role": role, "content": content})

def retrieve_docs(query: str, k: int = 3) -> List[str]:
    # TODO: é€™è£¡å¯ä»¥æ¥è³‡æ–™åº«æˆ–æª”æ¡ˆæª¢ç´¢
    return [f"[doc{i}] æ¨¡æ“¬æ–‡ä»¶å…§å®¹ï¼š{query}" for i in range(1, k+1)]

def format_responses_for_judge(responses: Dict[str, str]) -> str:
    lines = []
    for key, value in responses.items():
        lines.append(f"{key}: {value}")
    return "\n".join(lines)

# ---- Prompt templates ----
def lawyer_template(user_question: str) -> str:
    system_prompt = "å‡è¨­ä½ æ˜¯ä¸€åå¾‹å¸«ã€‚è«‹æ ¹æ“šé¦™æ¸¯æ³•ä¾‹å¾å°ˆæ¥­è§’åº¦å›ç­”ç”¨æˆ¶çš„å•é¡Œä¸¦è§£é‡‹ä½ çš„æ¨ç†ã€‚é™åˆ¶ï¼šç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚ä¸éœ€è¦å¼•å…¥æ¡ˆä¾‹ï¼Œä¸éœ€è¦å‡è¨­ä¸å­˜åœ¨çš„äº‹å¯¦ã€‚è«‹çµ¦æˆ‘ä¹¾æ·¨çš„å›ç­”ï¼Œå’Œä½¿ç”¨é»åˆ—æ–¹å¼è¼¸å‡ºå›è¦†ã€‚ç²—é«”è«‹ç”¨<b>...</b>æ¨™è¨˜ã€‚"
    return f"{system_prompt}\n{user_question}"

def contract_template(user_question: str, ) -> str:
    system_prompt = "ä½ æ˜¯ä¸€åå¾‹å¸«ã€‚è«‹æ ¹æ“šé¦™æ¸¯æ³•ä¾‹å¾å°ˆæ¥­è§’åº¦åˆ†æç”¨æˆ¶æä¾›çš„æ–‡ä»¶ï¼Œå…¶é¢¨éšªã€éŒ¯èª¤æˆ–éºæ¼ï¼Œä¸¦è©¢å•ç”¨æˆ¶éœ€è¦ä»€éº¼å¹«åŠ©ã€‚é™åˆ¶ï¼šç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚è«‹çµ¦æˆ‘ä¹¾æ·¨çš„å›ç­”ï¼Œå’Œä½¿ç”¨é»åˆ—æ–¹å¼è¼¸å‡ºå›è¦†ã€‚ç²—é«”è«‹ç”¨<b>...</b>æ¨™è¨˜ã€‚"
    return f"{system_prompt}\nç”¨æˆ¶æä¾›æ–‡ä»¶ï¼š{user_question}"

def prosecutor_template(user_question: str) -> str:
    system_prompt = "å‡è¨­ä½ æ˜¯ä¸€åå¾‹å¸«ã€‚è«‹æ ¹æ“šé¦™æ¸¯æ³•ä¾‹å¾å°ˆæ¥­è§’åº¦å›ç­”ç”¨æˆ¶çš„å•é¡Œä¸¦è§£é‡‹ä½ çš„æ¨ç†ä¸¦è§£é‡‹ä½ çš„æ¨ç†ã€‚é™åˆ¶ï¼šç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚ä¸éœ€è¦å¼•å…¥æ¡ˆä¾‹ï¼Œä¸éœ€è¦å‡è¨­ä¸å­˜åœ¨çš„äº‹å¯¦ã€‚è«‹çµ¦æˆ‘ä¹¾æ·¨çš„å›ç­”ï¼Œå’Œä½¿ç”¨é»åˆ—æ–¹å¼è¼¸å‡ºå›è¦†ã€‚ç²—é«”è«‹ç”¨<b>...</b>æ¨™è¨˜ã€‚"
    return f"{system_prompt}\n{user_question}"

def judge_template(user_question: str, responses: Dict[str, str]) -> str:
    system_prompt = "å‡è¨­ä½ æ˜¯ä¸€åæ³•å®˜ã€‚ç¸½çµå¤šè¼ªä¸­é›™æ–¹çš„è§€é»ï¼Œä¸¦æä¾›çµè«–ã€‚é™åˆ¶ï¼šåªéœ€è¦æä¾›çµè«–ï¼Œä¸éœ€è¦è¼¸å‡ºç¸½çµï¼Œç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸éœ€è¦å¼•å…¥æ¡ˆä¾‹ï¼Œä¸éœ€è¦å‡è¨­ä¸å­˜åœ¨çš„äº‹å¯¦ã€‚è«‹çµ¦æˆ‘ä¹¾æ·¨çš„å›ç­”ï¼Œå’Œä½¿ç”¨é»åˆ—æ–¹å¼è¼¸å‡ºå›è¦†ã€‚ç²—é«”è«‹ç”¨<b>...</b>æ¨™è¨˜ã€‚ä½ å¿…é ˆä»¥ã€éå°ˆæ¥­æ³•å¾‹æ„è¦‹ï¼Œå¦‚éœ€è¦æ³•å¾‹æ´åŠ©è«‹å°‹æ±‚å°ˆé–€äººå£«å”åŠ©ã€‚ã€çµæŸæ¯å€‹å›ç­”ã€‚"
    return f"{system_prompt}\nç”¨æˆ¶å•é¡Œï¼š{user_question}\nå¾‹å¸«å€‘çš„è§€é»ï¼š{format_responses_for_judge(responses)}"

def Guide_template(user_question: str, memory: Memory) -> str:
    history = "\n".join([f"{m['role']}: {m['content']}" for m in memory.messages[-6:]])
    system_prompt = "ä½ æ˜¯ä¸€å€‹åå«å°å¾‹çš„æ³•å¾‹é¡§å•åŠ©æ‰‹ã€‚ç¦®è²Œåœ°å•å€™ç”¨æˆ¶ä¸¦ä»‹ç´¹è‡ªå·±ã€‚ä¸¦å›ç­”ç”¨æˆ¶æœ‰é—œé¦™æ¸¯æ³•å¾‹çš„ç–‘å•ã€‚é™åˆ¶ï¼šä½ å¿…é ˆä»¥ã€éå°ˆæ¥­æ³•å¾‹æ„è¦‹ï¼Œå¦‚éœ€è¦æ³•å¾‹æ´åŠ©è«‹å°‹æ±‚å°ˆé–€äººå£«å”åŠ©ã€‚ã€çµæŸæ¯å€‹å›ç­”ã€‚ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚è«‹çµ¦æˆ‘ä¹¾æ·¨çš„å›ç­”ï¼Œå’Œä½¿ç”¨é»åˆ—æ–¹å¼è¼¸å‡ºå›è¦†ã€‚ç²—é«”è«‹ç”¨<b>...</b>æ¨™è¨˜ã€‚"
    return f"{system_prompt}\næ­·å²ï¼š{history}\nç”¨æˆ¶å•é¡Œï¼š{user_question}"

# ---- Agents ----
class BaseAgent:
    name = "base"
    def run(self, text: str, memory: Memory) -> str:
        raise NotImplementedError

class lawyerAgent(BaseAgent):
    name = "Lawyer"
    def run(self, text: str, memory: Memory) -> str:
        memory.add("user", text)   # å…ˆè¨˜éŒ„ç”¨æˆ¶è¼¸å…¥
        prompt = lawyer_template(text)
        # llm_generate is async (calls Vertex), run in asyncio loop if needed
        raw = asyncio.run(llm_generate(prompt, max_new_tokens=256))
        answer = extract_answer(raw)
        answer = sanitize_output(answer)
        memory.add(self.name, answer)  # å†è¨˜éŒ„ agent è¼¸å‡º
        return answer

class contractAgent(BaseAgent):
    name = "Contract"
    def run(self, text: str, memory: Memory) -> str:
        memory.add("user", text)
        prompt = contract_template(text)
        raw = asyncio.run(llm_generate(prompt, max_new_tokens=256))
        answer = extract_answer(raw)
        answer = sanitize_output(answer)
        memory.add(self.name, answer)
        return answer

class prosecutorAgent(BaseAgent):
    name = "Prosecutor"
    def run(self, text: str, memory: Memory) -> str:
        memory.add("user", text)
        prompt = prosecutor_template(text)
        raw = asyncio.run(llm_generate(prompt, max_new_tokens=256))
        answer = extract_answer(raw)
        answer = sanitize_output(answer)
        memory.add(self.name, answer)
        return answer

class JudgeAgent(BaseAgent):
    name = "Judge"
    def run(self, text: str, responses: Dict[str, str], memory: Memory) -> str:
        memory.add("user", text)
        prompt = judge_template(text, responses)
        raw = asyncio.run(llm_generate(prompt, max_new_tokens=256))
        answer = extract_answer(raw)
        answer = sanitize_output(answer)
        memory.add(self.name, answer)
        return answer

class guideAgent(BaseAgent):
    name = "Guide"
    def run(self, text: str, memory: Memory) -> str:
        memory.add("user", text)
        prompt = Guide_template(text, memory)   # âš ï¸ Guide_template è¦æ”¹æˆåªæ¥å—ä¸€å€‹åƒæ•¸
        raw = asyncio.run(llm_generate(prompt, max_new_tokens=256))
        answer = extract_answer(raw)
        answer = sanitize_output(answer)
        memory.add(self.name, answer)
        return answer

    
# ---- Router / Planner ----
AGENTS = {
    "Lawyer": lawyerAgent(),
    "Contract": contractAgent(),
    "Prosecutor": prosecutorAgent(),
    "Negotiate": None,           # ç”± orchestrator è™•ç†é›™å›åˆ + æ³•å®˜
    "Guide": guideAgent(),
    "Judge": JudgeAgent()
}

def route_task(text: str, has_contract: bool = False) -> str:
    """
    Router: æ ¹æ“šè¼¸å…¥æƒ…å¢ƒæ±ºå®šè¦èµ°å“ªå€‹ agent
    """
    t = text.lower()

    # 1. å¦‚æœæœ‰ä¸Šå‚³åˆç´„ PDF â†’ ContractAgent
    if has_contract or len(text) > 50:
        return "Contract"

    # 2. å¦‚æœæ˜¯æ³•å¾‹ç›¸é—œå•é¡Œ â†’ Negotiate
    if any(k in t for k in ["æ³•å¾‹", "åˆç´„", "åˆåŒ", "è¨´è¨Ÿ", "æ³•å®˜", "å¾‹å¸«", "æª¢æ§", "èµ·è¨´", "è¾¯è­·", "éºå›‘", "éºç”¢", "ç§Ÿç´„", "çŠ¯æ³•", "æ³•å¾‹", "æ³•ä¾‹", "è¦å®š", "è²¬ä»»", "æ¬Šåˆ©", "ç¾©å‹™", "è³ å„Ÿ", "ç´¢å„Ÿ", "ç³¾ç´›", "èª¿è§£", "ä»²è£", "è¨´ç‹€", "é•æ³•", "é•å"]):
        return "Negotiate"
    
    # 3. å…¶ä»–æƒ…æ³ â†’ GuideAgentï¼ˆå¼•å°ç”¨æˆ¶å•æ³•å¾‹å•é¡Œï¼‰
    return "Guide"

# ---- Orchestrator ----
def orchestrate(text: str, memory: Memory) -> Dict[str, Any]:
    start = time.time()
    agent_name = route_task(text)
    if agent_name == "Negotiate":
        return {"agent_used": "Negotiate", "result": None, "latency_sec": 0.0}

    agent = AGENTS[agent_name]
    memory.add("user", text)

    result = agent.run(text, memory)
    memory.add(agent_name, result)

    elapsed = round(time.time() - start, 3)
    return {"agent_used": agent_name, "result": result, "latency_sec": elapsed}

def negotiate_stream(user_question: str, memory: Memory, max_rounds: int = 3):
    responses: Dict[str, str] = {}

    # ç¬¬ä¸€è¼ª
    lawyer_r1 = lawyerAgent().run(user_question, memory)
    responses["lawyer_r1"] = lawyer_r1
    yield f"data: {json.dumps({'agent': 'Lawyer', 'round': 1, 'output': lawyer_r1}, ensure_ascii=False)}\n\n"

    prosecutor_r1 = prosecutorAgent().run(user_question, memory)
    responses["prosecutor_r1"] = prosecutor_r1
    yield f"data: {json.dumps({'agent': 'Prosecutor', 'round': 1, 'output': prosecutor_r1}, ensure_ascii=False)}\n\n"

    if "æ²’æœ‰æ„è¦‹" in prosecutor_r1 or "æˆ‘åŒæ„è¾¯æ–¹å¾‹å¸«" in prosecutor_r1:
        judge_result = JudgeAgent().run(user_question, responses, memory)
        yield f"data: {json.dumps({'agent': 'Judge', 'output': judge_result}, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"
        return

    # å¾ŒçºŒå¤šè¼ª
    for r in range(2, max_rounds + 1):
        lawyer_reply = lawyerAgent().run(f"æ§æ–¹å¾‹å¸«ä¸Šä¸€è¼ªçš„æ„è¦‹æ˜¯ï¼š{responses[f'prosecutor_r{r-1}']}", memory)
        responses[f"lawyer_r{r}"] = lawyer_reply
        yield f"data: {json.dumps({'agent': 'Lawyer', 'round': r, 'output': lawyer_reply}, ensure_ascii=False)}\n\n"

        prosecutor_reply = prosecutorAgent().run(f"è¾¯è­·å¾‹å¸«ä¸Šä¸€è¼ªçš„æ„è¦‹æ˜¯ï¼š{responses[f'lawyer_r{r-1}']}", memory)
        responses[f"prosecutor_r{r}"] = prosecutor_reply
        yield f"data: {json.dumps({'agent': 'Prosecutor', 'round': r, 'output': prosecutor_reply}, ensure_ascii=False)}\n\n"

        if "æ²’æœ‰æ„è¦‹" in prosecutor_reply or "åŒæ„" in prosecutor_reply:
            break

    # æ³•å®˜ç¸½çµ
    judge_result = JudgeAgent().run(user_question, responses, memory)
    yield f"data: {json.dumps({'agent': 'Judge', 'output': judge_result}, ensure_ascii=False)}\n\n"
    yield "data: [DONE]\n\n"


@app.get("/")
async def root():
    return {"status": "ok"}

@app.get("/health")
async def health():
    return {"status": "healthy"}

@app.post("/predict")
async def predict(request: Request):
    body = await request.json()

    # æ”¯æ´å…©ç¨®è¼¸å…¥æ ¼å¼ï¼š
    # 1) èˆŠçš„ instances æ ¼å¼ï¼š{"instances":[{"text":"..."}], "has_contract": false}
    # 2) æ–°çš„ system_prompt + user_question æ ¼å¼ï¼š{"system_prompt": "...", "user_question": "..."}
    prompt = None
    has_contract = False
    override_agent = None

    # agent è¦†å¯«ï¼ˆoptionalï¼‰
    if isinstance(body, dict):
        override_agent = body.get("agent")

    if isinstance(body, dict) and "system_prompt" in body and "user_question" in body:
        # ä½¿ç”¨æ˜ç¢ºçš„ system_prompt + user_question
        system_prompt = body.get("system_prompt") or ""
        user_question = body.get("user_question") or ""
        prompt = f"{system_prompt}\n{user_question}".strip()
        has_contract = body.get("has_contract", False)
    else:
        # fallback to instances format for backward compatibility
        instances = body.get("instances", []) if isinstance(body, dict) else []
        if not instances or "text" not in instances[0]:
            def empty():
                yield f"data: {json.dumps({'error': 'æ²’æœ‰è¼¸å…¥æ–‡å­—'}, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
            return StreamingResponse(empty(), media_type="text/event-stream")

        prompt = instances[0]["text"]
        has_contract = body.get("has_contract", False)

    memory = Memory()

    def event_stream():
        # allow override of routing (e.g., pass "agent":"Guide" in JSON to force)
        routed = override_agent if override_agent else route_task(prompt, has_contract)

        if routed == "Contract":
            out = contractAgent().run(prompt, memory)
            yield f"data: {json.dumps({'agent': 'Contract', 'output': out}, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
            return

        if routed == "Guide":
            out = guideAgent().run(prompt, memory)
            yield f"data: {json.dumps({'agent': 'Guide', 'output': out}, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
            return

        if routed == "Negotiate":
            yield from negotiate_stream(prompt, memory)
            return

        # é è¨­ï¼šLawyer
        out = lawyerAgent().run(prompt, memory)
        yield f"data: {json.dumps({'agent': 'Lawyer', 'output': out}, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")


