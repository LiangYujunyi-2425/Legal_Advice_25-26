from transformers import AutoTokenizer
from transformers import Gemma3ForConditionalGeneration  # éœ€è¦æœ€æ–° transformers
from peft import PeftModel

base_model_path = "./models/gemma-3-4b-it"      # ä½ çš„ Gemma3 base model
adapter_model_path = "./smellts"  # LoRA adapter
save_path = "./merged-llm"        # è¼¸å‡ºåˆä½µå¾Œçš„æ¨¡å‹

print("ğŸ”¹ è¼‰å…¥ base model...")
model = Gemma3ForConditionalGeneration.from_pretrained(
    base_model_path,
    torch_dtype="auto",
    device_map="auto"
)

print("ğŸ”¹ å¥—ç”¨ LoRA adapter...")
model = PeftModel.from_pretrained(model, adapter_model_path)

print("ğŸ”¹ åˆä½µ LoRA æ¬Šé‡...")
model = model.merge_and_unload()

print("ğŸ”¹ å„²å­˜åˆä½µå¾Œçš„æ¨¡å‹...")
model.save_pretrained(save_path)

tokenizer = AutoTokenizer.from_pretrained(base_model_path)
tokenizer.save_pretrained(save_path)

print(f"âœ… å®Œæˆï¼åˆä½µå¾Œçš„æ¨¡å‹å·²å„²å­˜åˆ° {save_path}")