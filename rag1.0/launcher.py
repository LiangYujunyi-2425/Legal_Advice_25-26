import streamlit as st
import subprocess
import threading
import time
import os
import requests
import sys

OLLAMA_CMD = "ollama"
OLLAMA_DOWNLOAD_URL = "https://ollama.com/download/OllamaSetup.exe"
OLLAMA_INSTALLER = "OllamaSetup.exe"
OLLAMA_API_URL = "http://localhost:11434/api/tags"

# é è¨­ Ollama çš„æ¨¡å‹å¿«å–ç›®éŒ„
OLLAMA_MODELS_DIR = os.path.expanduser("~/.ollama/models")


def is_ollama_installed():
    try:
        result = subprocess.run([OLLAMA_CMD, "--version"],
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False


def install_ollama():
    st.warning("âš ï¸ æª¢æŸ¥åˆ°æœªå®‰è£ Ollamaï¼Œæ­£åœ¨ä¸‹è¼‰å®‰è£æª”...")
    if not os.path.exists(OLLAMA_INSTALLER):
        try:
            with requests.get(OLLAMA_DOWNLOAD_URL, stream=True) as r:
                r.raise_for_status()
                with open(OLLAMA_INSTALLER, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)
            st.success(f"âœ… å·²ä¸‹è¼‰ Ollama å®‰è£æª”ï¼š{OLLAMA_INSTALLER}")
        except Exception as e:
            st.error(f"âŒ ä¸‹è¼‰ Ollama å¤±æ•—: {e}")
            return False

    st.info("ğŸ“¦ è«‹æ‰‹å‹•åŸ·è¡Œå®‰è£ç¨‹å¼ï¼Œå®‰è£å®Œæˆå¾Œé‡æ–°å•Ÿå‹•å•Ÿå‹•å™¨ã€‚")
    os.startfile(OLLAMA_INSTALLER)
    return False


def wait_for_ollama():
    """ç­‰å¾… Ollama API å•Ÿå‹•"""
    for _ in range(30):
        try:
            resp = requests.get(OLLAMA_API_URL, timeout=2)
            if resp.status_code == 200:
                return True
        except requests.exceptions.RequestException:
            pass
        time.sleep(1)
    return False


def list_installed_models():
    try:
        result = subprocess.run([OLLAMA_CMD, "list"],
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        models = []
        for line in result.stdout.strip().split("\n")[1:]:
            if line:
                models.append(line.split()[0])
        return models
    except Exception:
        return []


def is_model_cached(model_name: str) -> bool:
    """æª¢æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ¨¡å‹å¿«å–æª”"""
    if not os.path.exists(OLLAMA_MODELS_DIR):
        return False

    for root, dirs, files in os.walk(OLLAMA_MODELS_DIR):
        for file in files:
            if model_name.replace(":", "_") in file:  # qwen3:8b â†’ qwen3_8b
                return True
    return False


def pull_model(model_name):
    """ä¸‹è¼‰æ¨¡å‹ï¼Œä¸¦é¡¯ç¤ºé€²åº¦æ¢"""
    st.info(f"ğŸ“¥ æ­£åœ¨å®‰è£æ¨¡å‹ {model_name} ...")
    progress_bar = st.progress(0)
    status_text = st.empty()

    process = subprocess.Popen(
        [OLLAMA_CMD, "pull", model_name],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        encoding="utf-8",   # âœ… ç”¨ UTF-8 è§£ç¢¼
        errors="ignore"     # âœ… é‡åˆ°ç„¡æ³•è§£ç¢¼çš„å­—å…ƒç›´æ¥å¿½ç•¥
    )

    line_count = 0
    for line in process.stdout:
        line_count += 1
        status_text.text(line.strip())
        progress_bar.progress(min(line_count % 100, 100) / 100.0)

    process.wait()
    if process.returncode == 0:
        progress_bar.progress(1.0)
        st.success(f"âœ… æ¨¡å‹ {model_name} å®‰è£å®Œæˆ")
        return True
    else:
        st.error(f"âŒ æ¨¡å‹ {model_name} å®‰è£å¤±æ•—")
        return False


def run_ollama():
    st.write("ğŸŸ¢ å•Ÿå‹• Ollama server ...")
    subprocess.Popen([OLLAMA_CMD, "serve"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)


def run_web_ui():
    st.write("ğŸŸ¢ å•Ÿå‹• Web UI ...")
    subprocess.Popen(
        [sys.executable, "-m", "streamlit", "run", "web_contract_ui_local.py"],
        shell=True
    )


# -------------------- Streamlit UI --------------------
st.title("ğŸš€ æœ¬åœ° AI å•Ÿå‹•å™¨")

if not is_ollama_installed():
    st.error("âŒ å°šæœªå®‰è£ Ollama")
    if st.button("ä¸‹è¼‰ä¸¦å®‰è£ Ollama"):
        install_ollama()
    st.stop()

COMMON_MODELS = ["qwen3:8b", "qwen3:4b", "llama3:8b", "mistral:7b"]
installed_models = list_installed_models()
model_choice = st.selectbox("è«‹é¸æ“‡è¦å•Ÿå‹•çš„ Ollama æ¨¡å‹ï¼š", COMMON_MODELS)

if st.button("å•Ÿå‹•ç³»çµ±"):
    st.success(f"å·²é¸æ“‡æ¨¡å‹ï¼š{model_choice}")

    threading.Thread(target=run_ollama, daemon=True).start()

    if wait_for_ollama():
        st.success("âœ… Ollama API å·²å•Ÿå‹•ï¼")

        if model_choice not in installed_models and not is_model_cached(model_choice):
            ok = pull_model(model_choice)
            if not ok:
                st.stop()
            else:
                st.info("ğŸ”„ æ¨¡å‹å®‰è£å®Œæˆï¼Œæ­£åœ¨é‡å•Ÿ Web UI...")
                time.sleep(2)

        threading.Thread(target=run_web_ui, daemon=True).start()

    else:
        st.error("âŒ Ollama API å•Ÿå‹•å¤±æ•—ï¼Œè«‹æª¢æŸ¥å®‰è£æ˜¯å¦æ­£å¸¸ã€‚")

