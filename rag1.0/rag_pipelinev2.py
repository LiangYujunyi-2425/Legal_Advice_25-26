import os
import pickle
import requests
import chromadb
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
import jieba
from dotenv import load_dotenv

# ================== ç’°å¢ƒè®Šæ•¸ ==================
load_dotenv()

# ================== Embedding Function ==================
class GTEEmbeddingFunction:
    def __init__(self, model_name="thenlper/gte-large-zh"):
        print(f"ğŸ“¥ è¼‰å…¥æœ¬åœ°æ¨¡å‹ {model_name} ...")
        self.model = SentenceTransformer(model_name)
        self.model_name = model_name

    def __call__(self, input: list[str]) -> list[list[float]]:
        return self.model.encode(input, show_progress_bar=False).tolist()

    def name(self) -> str:
        return f"sentence-transformers/{self.model_name}"

# åˆå§‹åŒ–æ¨¡å‹
embedder = SentenceTransformer("thenlper/gte-large-zh")
reranker = CrossEncoder("BAAI/bge-reranker-large")

# ================== Chroma åˆå§‹åŒ– ==================
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(
    name="hk_cap4_laws",
    embedding_function=GTEEmbeddingFunction("thenlper/gte-large-zh")
)

# ================== BM25 (è¼‰å…¥ç´¢å¼•) ==================
if not os.path.exists("bm25_index.pkl"):
    raise FileNotFoundError("âŒ æ‰¾ä¸åˆ° bm25_index.pklï¼Œè«‹å…ˆåŸ·è¡Œ batch_cap4.py")

with open("bm25_index.pkl", "rb") as f:
    bm25_data = pickle.load(f)
bm25 = bm25_data["bm25"]
bm25_chunks = bm25_data["chunks"]

# ================== Gemini API ==================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("âŒ è«‹å…ˆåœ¨ .env è¨­å®š GEMINI_API_KEY")

GEMINI_MODEL = "models/gemini-2.5-flash"
GEMINI_ENDPOINT = f"https://generativelanguage.googleapis.com/v1beta/{GEMINI_MODEL}:generateContent"
HEADERS = {"Content-Type": "application/json", "x-goog-api-key": GEMINI_API_KEY}

# ================== æª¢ç´¢ ==================
def hybrid_search(query: str, n=10):
    # 1. å‘é‡æª¢ç´¢
    vector_results = collection.query(
        query_texts=[query],
        n_results=n,
        include=["documents", "metadatas", "distances"]
    )
    vector_docs = vector_results["documents"][0]
    vector_metas = vector_results["metadatas"][0]
    vector_scores = vector_results["distances"][0]

    vector_candidates = [
        (doc, meta, 1 - score, "Chroma")
        for doc, meta, score in zip(vector_docs, vector_metas, vector_scores)
    ]

    # 2. BM25 æª¢ç´¢
    tokenized_query = list(jieba.cut(query))
    bm25_scores = bm25.get_scores(tokenized_query)
    top_idx = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:n]

    bm25_candidates = [
        (bm25_chunks[i]["text"], bm25_chunks[i], bm25_scores[i], "BM25") for i in top_idx
    ]

    # 3. åˆä½µ
    merged = {}
    for doc, meta, score, source in vector_candidates + bm25_candidates:
        if doc not in merged or score > merged[doc][1]:
            merged[doc] = (meta, score, source)

    return [(doc, meta, score, source) for doc, (meta, score, source) in merged.items()]

# ================== Rerank ==================
def rerank(query, candidates, top_k=3, debug=False):
    pairs = [(query, doc) for doc, _, _, _ in candidates]
    scores = reranker.predict(pairs)

    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)

    if debug:
        print("\nğŸ“Š Debug - å€™é¸æª¢ç´¢åˆ†æ•¸å°æ¯”ï¼š")
        for (doc, meta, base_score, source), rerank_score in ranked[:10]:
            print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            print(f"ä¾†æº: {source}")
            print(f"æ³•ä¾‹: {meta.get('law_name','')} {meta.get('section','')}")
            print(f"åˆå§‹åˆ†æ•¸: {base_score:.4f}")
            print(f"Reranker åˆ†æ•¸: {rerank_score:.4f}")
            print(f"æ¢æ–‡å…§å®¹: {doc[:80]}...")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    return ranked[:top_k]

# ================== Gemini å›ç­” ==================
def call_gemini(prompt):
    body = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0.2, 
            "maxOutputTokens": 4096,
            "topP": 0.8,
            "topK": 10
        }
    }

    try:
        resp = requests.post(GEMINI_ENDPOINT, headers=HEADERS, json=body, timeout=30)
        
        if resp.status_code != 200:
            print(f"âŒ HTTP éŒ¯èª¤ {resp.status_code}: {resp.text}")
            return f"âš ï¸ Gemini API è«‹æ±‚å¤±æ•—: {resp.status_code}"

        resp_data = resp.json()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
        if "error" in resp_data:
            print(f"âŒ API éŒ¯èª¤: {resp_data['error']}")
            return f"âš ï¸ Gemini API éŒ¯èª¤: {resp_data['error'].get('message', 'æœªçŸ¥éŒ¯èª¤')}"
        
        # æª¢æŸ¥å€™é¸å›ç­”
        if "candidates" not in resp_data or not resp_data["candidates"]:
            print(f"âŒ ç„¡å€™é¸å›ç­”: {resp_data}")
            return "âš ï¸ Gemini æœªè¿”å›ä»»ä½•å€™é¸å›ç­”"
        
        candidate = resp_data["candidates"][0]
        
        # æª¢æŸ¥æ˜¯å¦è¢«å®‰å…¨éæ¿¾å™¨é˜»æ“‹
        if "finishReason" in candidate and candidate["finishReason"] != "STOP":
            print(f"âŒ å›ç­”è¢«é˜»æ“‹: {candidate.get('finishReason')}")
            return f"âš ï¸ å›ç­”è¢«å®‰å…¨éæ¿¾å™¨é˜»æ“‹: {candidate.get('finishReason')}"
        
        # æå–æ–‡å­—å…§å®¹
        if "content" in candidate and "parts" in candidate["content"]:
            parts = candidate["content"]["parts"]
            if parts and "text" in parts[0]:
                return parts[0]["text"]
        
        print(f"âŒ ç„¡æ³•è§£æå›ç­”: {resp_data}")
        return "âš ï¸ ç„¡æ³•è§£æ Gemini å›ç­”æ ¼å¼"
        
    except requests.exceptions.Timeout:
        return "âš ï¸ Gemini API è«‹æ±‚è¶…æ™‚"
    except requests.exceptions.RequestException as e:
        return f"âš ï¸ ç¶²è·¯è«‹æ±‚éŒ¯èª¤: {e}"
    except Exception as e:
        print(f"âŒ æœªé æœŸéŒ¯èª¤: {e}")
        return f"âš ï¸ è™•ç† Gemini å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"

def generate_answer_with_review(query, context_texts, sources):
    # ç°¡åŒ–ç‚ºå–®æ¬¡èª¿ç”¨ï¼Œé¿å… token éå¤š
    prompt = (
        "ä½ æ˜¯é¦™æ¸¯æ³•å¾‹è¼”åŠ©åŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ³•å¾‹æ¢æ–‡å›ç­”å•é¡Œã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”\n"
        "2. ç°¡æ½”æº–ç¢ºï¼Œé‡é»çªå‡º\n"
        "3. é€™ä¸æ˜¯æ³•å¾‹æ„è¦‹ï¼Œåƒ…ä¾›åƒè€ƒ\n\n"
        f"ç›¸é—œæ³•å¾‹æ¢æ–‡ï¼š\n{chr(10).join(context_texts[:3])}\n\n"  # é™åˆ¶æ¢æ–‡æ•¸é‡
        f"å•é¡Œï¼š{query}\n\n"
        "è«‹å›ç­”ï¼š"
    )
    
    answer = call_gemini(prompt)
    return f"{answer}\n\nğŸ“š ä¾†æºï¼š\n" + "\n".join(sources)

# ================== ä¸»ç¨‹å¼ ==================
if __name__ == "__main__":
    print("âœ… Hybrid RAG Pipeline 2.1 å·²å•Ÿå‹•ï¼ˆè¼¸å…¥ exit é›¢é–‹ï¼‰")

    while True:
        query = input("\nâ“ è«‹è¼¸å…¥å•é¡Œ: ").strip()
        if query.lower() in ["exit", "quit", "q"]:
            print("ğŸ‘‹ å†è¦‹ï¼")
            break

        candidates = hybrid_search(query, n=10)
        reranked = rerank(query, candidates, top_k=3, debug=True)

        context_texts = [doc for (doc, _, _, _), _ in reranked]
        sources = [f"- {meta.get('law_name','')} {meta.get('section','')}" for (_, meta, _, _), _ in reranked]

        answer = generate_answer_with_review(query, context_texts, sources)
        print("\nğŸ§  Gemini æœ€çµ‚å›ç­”ï¼š")
        print(answer)